{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RL Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-06T19:59:12.951125Z",
     "start_time": "2021-12-06T19:59:09.075353Z"
    }
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "import matplotlib.patches as mpatches\n",
    "from enum import IntEnum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment & MDP methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-07T00:34:49.714402Z",
     "start_time": "2021-12-07T00:34:49.640638Z"
    }
   },
   "outputs": [],
   "source": [
    "class Action(IntEnum):\n",
    "    UP = 0\n",
    "    DOWN = 1\n",
    "    LEFT = 2\n",
    "    RIGHT = 3\n",
    "    STAY = 4\n",
    "\n",
    "class MDP:\n",
    "    \n",
    "    # How to use :\n",
    "    #\n",
    "    # Use mdp.call(state, action) to get info on the environment.\n",
    "    # --> Do NOT use T & R matrices to get info on the environnement.\n",
    "    # --> You CAN use mdp.utils methods, as long as it has a update_n_calls parameter.\n",
    "    # Use mdp.step(action) to move the agent .\n",
    "    # Use mdp.V to store & update state values.\n",
    "    # Use mdp.n_calls to get the number of calls made to the environnement.\n",
    "    # Use mdp.reset() when the episode has ended to start over.\n",
    "\n",
    "    def __init__(self, N = 5, discount_factor = 0.99, reward_range = 0.1): # discount_factor's value isn't specified in paper\n",
    "        self.N = N\n",
    "        self.n_states = N * N\n",
    "        self.discount_factor = discount_factor\n",
    "        self.reward_range = reward_range\n",
    "        self.reset()\n",
    "        self.T = self.generate_transition_matrix(N)\n",
    "        self.displayer = MDP_Displayer(self)\n",
    "        self.utils = MDP_Utils(self)\n",
    "    \n",
    "    def generate_transition_matrix(self, N):\n",
    "        transition_matrix = np.zeros((N * N, len(Action)), dtype=int)\n",
    "        for s, state in enumerate(transition_matrix):\n",
    "            state[Action.UP] = s - N if s >= N else s\n",
    "            state[Action.DOWN] = s + N if s < N * N - N else s\n",
    "            state[Action.LEFT] = s - 1 if s % N else s\n",
    "            state[Action.RIGHT] = s + 1 if (s + 1) % N else s\n",
    "            state[Action.STAY] = s\n",
    "        return transition_matrix\n",
    "    \n",
    "    def generate_rewards(self, N, reward_range):\n",
    "        rewards = np.random.uniform(-reward_range, reward_range, N * N)\n",
    "        rewards[np.random.randint(0, N * N)] = 1\n",
    "        return rewards\n",
    "    \n",
    "    def generate_state_values(self, N):\n",
    "        return np.random.normal(size = N * N)\n",
    "    \n",
    "    def reset(self):\n",
    "        self.n_calls = 0\n",
    "        self.state = 0 # Paper does no mention what state to start on: 0 or random ?\n",
    "        self.R = self.generate_rewards(self.N, self.reward_range)\n",
    "        self.V = self.generate_state_values(self.N)\n",
    "    \n",
    "    def step(self, action):\n",
    "        self.state = self.T[self.state, action]\n",
    "        return self.state, self.R[self.state]\n",
    "    \n",
    "    def call(self, state, action):\n",
    "        n_calls += 1\n",
    "        new_state = self.T[state, action]\n",
    "        return new_state, self.R[new_state]\n",
    "        \n",
    "class MDP_Displayer:\n",
    "    \n",
    "    def __init__(self, mdp):\n",
    "        self.mdp = mdp\n",
    "        self.set_action_colors()\n",
    "    \n",
    "    def display_rewards(self, numerical = False, colorbar = True, ticks = False, title = \"Rewards\"):\n",
    "        self.display_heatmap(self.mdp.R, numerical, colorbar, ticks, title)\n",
    "    \n",
    "    def display_state_values(self, numerical = False, colorbar = True, ticks = False, title = \"State values\"):\n",
    "        self.display_heatmap(self.mdp.V, numerical, colorbar, ticks, title)\n",
    "        \n",
    "    def display_actions_based_on_rewards(self, ticks = False, title = \"Best actions based on rewards\"):\n",
    "        self.display_actions(self.mdp.utils.get_best_actions_based_on_rewards(False), ticks, title)\n",
    "    \n",
    "    def display_actions_based_on_state_values(self, ticks = False, title = \"Best actions based on state values\"):\n",
    "        self.display_actions(self.mdp.utils.get_best_actions_based_on_state_values(), ticks, title)\n",
    "    \n",
    "    def display_heatmap(self, array, numerical = False, colorbar = True, ticks = False, title = \"Heatmap\"):\n",
    "        data = np.reshape(array, (self.mdp.N, self.mdp.N))\n",
    "        if not ticks: self.remove_ticks()\n",
    "        if numerical: self.add_numerical(data)\n",
    "        plt.imshow(data)\n",
    "        plt.title(title)\n",
    "        if colorbar: plt.colorbar()\n",
    "        plt.show()\n",
    "        \n",
    "    def display_actions(self, actions, ticks = False, title = \"Actions\"):  \n",
    "        self.add_action_legend()\n",
    "        data = np.reshape(actions, (self.mdp.N, self.mdp.N))\n",
    "        if not ticks: self.remove_ticks()\n",
    "        plt.imshow(data, cmap = self.action_cmap, norm = self.action_norm)\n",
    "        plt.title(title)\n",
    "        plt.show()\n",
    "    \n",
    "    def remove_ticks(self):\n",
    "        plt.gca().set_xticklabels([])\n",
    "        plt.gca().set_yticklabels([])\n",
    "        for tick in plt.gca().xaxis.get_major_ticks():\n",
    "            tick.tick1line.set_visible(False)\n",
    "            tick.tick2line.set_visible(False)\n",
    "        for tick in plt.gca().yaxis.get_major_ticks():\n",
    "            tick.tick1line.set_visible(False)\n",
    "            tick.tick2line.set_visible(False)\n",
    "    \n",
    "    # Adds numerical data inside each cell of the plot\n",
    "    def add_numerical(self, data):\n",
    "        image = plt.imshow(data)\n",
    "        threshold = image.norm(data.max()) * 2/3\n",
    "        for i in range(len(data)):\n",
    "            for j in range(len(data[i])):\n",
    "                color = int(image.norm(data[i, j]) < threshold)\n",
    "                text = plt.gca().text(j, i, round(data[i, j], 2),\n",
    "                                      ha = \"center\", va = \"center\", color = str(color))\n",
    "    \n",
    "    def add_action_legend(self):\n",
    "        handles = []\n",
    "        for i, action in enumerate(Action):\n",
    "            handles.append(mpatches.Patch(color = self.action_colors[i],\n",
    "                                          label = str(action)[str(action).find(\".\") + 1:]))\n",
    "        plt.legend(title=\"Actions\", handles = handles, loc='upper left', bbox_to_anchor=(1, 1))\n",
    "        \n",
    "    def set_action_colors(self):\n",
    "        self.action_colors = [\"red\", \"blue\", \"green\", \"yellow\", \"purple\"]\n",
    "        self.action_cmap = mcolors.ListedColormap(self.action_colors)\n",
    "        self.action_norm = mcolors.BoundaryNorm(list(range(len(Action) + 1)), self.action_cmap.N)\n",
    "        \n",
    "class MDP_Utils:\n",
    "    \n",
    "    def __init__(self, mdp):\n",
    "        self.mdp = mdp\n",
    "        \n",
    "    # GENERATIVE METHODS ========================================================================\n",
    "        \n",
    "    # Warning: depth > 9 will most likely crash your kernel\n",
    "    def get_action_trajecories(self, depth):\n",
    "        return np.array(np.meshgrid(*[Action] * depth)).T.reshape(-1, depth)\n",
    "    \n",
    "    def get_uniform_policy(self):\n",
    "        return np.ones((self.mdp.n_states, len(Action)), dtype=int) / len(Action)\n",
    "        \n",
    "    # INFORMATIVE METHODS =======================================================================\n",
    "    \n",
    "    def get_best_actions_based_on_rewards(self, update_n_calls = True):\n",
    "        return np.argmax(self.get_action_rewards(update_n_calls), 1)\n",
    "    \n",
    "    def get_best_actions_based_on_state_values(self):\n",
    "        return np.argmax(self.get_action_state_values(), 1)   \n",
    "    \n",
    "    # Return rewards array of size (n_state, actions)\n",
    "    def get_action_rewards(self, update_n_calls = True):\n",
    "        action_rewards = self.mdp.R[self.mdp.T]\n",
    "        if update_n_calls: self.mdp.n_calls += self.mdp.n_states * len(Action)\n",
    "        return action_rewards\n",
    "    \n",
    "    # Return state values array of size (n_state, actions), aka the value of the next state\n",
    "    def get_action_state_values(self):\n",
    "        action_state_values = self.mdp.V[self.mdp.T]\n",
    "        return action_state_values\n",
    "    \n",
    "    # CONVERSION METHODS ========================================================================\n",
    "        \n",
    "    def actions_to_greedy_policy(self, best_actions):\n",
    "        return np.eye(len(Action))[best_actions]\n",
    "    \n",
    "    def greedy_policy_to_actions(self, policy):\n",
    "        return np.argmax(policy, 1)\n",
    "    \n",
    "    # top_left=(0,0) any_coord=(row, column)\n",
    "    def state_to_coord(self, state):\n",
    "        return (int(np.floor(state / self.mdp.N)), state % self.mdp.N)\n",
    "    \n",
    "    # top_left=(0,0) any_coord=(row, column)\n",
    "    def coord_to_state(self, coord):\n",
    "        return coord[0] * self.mdp.N + coord[1]\n",
    "    \n",
    "    # BELLMAN METHODS =====================================================================\n",
    "    #\n",
    "    # check out this link for a better explanation of R, P, V & T than the paper's\n",
    "    # https://ai.stackexchange.com/questions/11057/what-is-the-bellman-operator-in-reinforcement-learning\n",
    "    #\n",
    "    # All the R, P = None is to make sure all functions can be called in a loop without recomputing R, P\n",
    "    # Same thing for returning R, P\n",
    "    \n",
    "    # Corresponds to bellman's P in the paper\n",
    "    def get_bellman_transition_kernel(self, policy):\n",
    "        P = np.zeros((self.mdp.n_states, self.mdp.n_states))\n",
    "        for s, state_transition in enumerate(self.mdp.T):\n",
    "            for action in Action:\n",
    "                P[s, state_transition[action]] += policy[s, action]\n",
    "        return P\n",
    "    \n",
    "    # Corresponds to bellman's R^π in the paper\n",
    "    def get_bellman_rewards(self, policy, update_n_calls = True):\n",
    "        self.mdp.n_calls += self.mdp.n_states * len(Action)\n",
    "        return np.sum(self.mdp.R[self.mdp.T] * policy, 1)\n",
    "    \n",
    "    # Corresponds to T^π in the paper\n",
    "    def bellman_operator(self, policy, R = None, P = None, V = None, update_n_calls = True):\n",
    "        if R is None: R = self.get_bellman_rewards(policy, update_n_calls)\n",
    "        if P is None: P = self.get_bellman_transition_kernel(policy)\n",
    "        if V is None: V = self.mdp.V\n",
    "        V = R + self.mdp.discount_factor * P.dot(V)\n",
    "        return R, P, V\n",
    "    \n",
    "    # Corresponds to T, T^x & (T^π)^x in the paper\n",
    "    def bellman_optimatlity_operator(self, policy = None, n_times = 1, R = None, P = None, V = None, update_n_calls = True):\n",
    "        if policy is None: policy = self.get_uniform_policy()\n",
    "        for i in range(n_times):\n",
    "            R, P, V = self.bellman_operator(policy, R, P, V, update_n_calls)\n",
    "            V = np.max(V[self.mdp.T], 1)\n",
    "        return R, P, V\n",
    "    \n",
    "    # Corresponds to G & G_h in the paper\n",
    "    def get_h_greedy_policy(self, h = 1, policy = None, R = None, P = None, V = None, update_n_calls = True):\n",
    "        if policy is None: policy = self.get_uniform_policy()\n",
    "        R, P, V = self.bellman_optimatlity_operator(policy, h, R, P, V, update_n_calls)\n",
    "        R, P, V = self.bellman_operator(policy, R, P, V, update_n_calls)\n",
    "        best_actions = np.argmax(V[mdp.T], 1)\n",
    "        policy = self.actions_to_greedy_policy(best_actions)\n",
    "        return policy, R, P, V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-07T00:40:14.234670Z",
     "start_time": "2021-12-07T00:40:14.206783Z"
    }
   },
   "outputs": [],
   "source": [
    "def value_or_policy_iteration(mdp = MDP()):\n",
    "    pass\n",
    "\n",
    "def h_PI(mdp = MDP(), h = 1, tolerance = 0.0):\n",
    "    if mdp.discount_factor < 0 or mdp.discount_factor >= 1:\n",
    "        raise ValueError(\"h_PI() will not converge unless discount_factor ∈ [0, 1[\")\n",
    "    policy, _, _, V = mdp.utils.get_h_greedy_policy(h = h)\n",
    "    Vs = [V]\n",
    "    while True:\n",
    "        policy, _, _, V = mdp.utils.get_h_greedy_policy(h = h, V = Vs[-1])\n",
    "        Vs.append(V)\n",
    "        if np.allclose(Vs[-1], Vs[-2], tolerance): break\n",
    "    return policy, Vs\n",
    "\n",
    "def NC_hm_PI(mdp = MDP(), h = 1, m = 1):\n",
    "    pass\n",
    "\n",
    "def hm_PI(mdp = MDP(), h = 1, m = 1):\n",
    "    pass\n",
    "\n",
    "def NC_hλ_PI(mdp = MDP(), h = 1, λ = 1):\n",
    "    pass\n",
    "\n",
    "def hλ_PI(mdp = MDP(), h = 1, λ = 1):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-07T01:10:37.227436Z",
     "start_time": "2021-12-07T01:10:36.085424Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAARoAAAD3CAYAAADCHptSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAASpUlEQVR4nO3df5BdZX3H8fdnN4QgkKAEB0jCLwmDgf6gxIhlFKxgg9OBtiM1OFZpUTqt1FZbHdoytINtbWkdO1a04kgROi1S6tiMTRvbKqiMaMJoqQlQ1iBmCQgJvwI2P3b30z/uDV4um3vPJufZ3HP7ec2cmXvuOfd7zmbZD8/znOecK9tERJQ0cqBPICKGX4ImIopL0EREcQmaiCguQRMRxSVoIqK4BE0UI+l2Se880OcRB16Cpg9J35O0S9LCrve/LcmSTmiv39jeb3t7+Y6kD0la0PGZSyVNSnpW0jPtGj/X3naupNtn8UeLmDUJmmoeBC7ZsyLpx4BDptnvWtuHA0cBvwKcBdwp6dCOfb5u+zDgCODTwK2SXlbHSapl1n+nkubM9jGjWRI01dwMvL1j/R3ATXvb2fYO2+uAC4EjaYVO9z5TwA20Auukzm3twPiIpMckPS3pHkmnT3esdvfkTyTdCfwQOEnSAkmflvSIpIcl/bGk0fb+D0k6s/36be1W2bL2+jslfb79eoWkr0t6ql3nY5LmdhzXkt4t6QHggfZ750u6r33OHwPUsf/Jku5ob9sq6bN7+/eL4ZOgqeYuYL6kV7b/YN8C/F2/D9neDvw78Nrube1WwDuBZ4EHbN9u+9z25jcCrwNOodXyeQuwrcehfhm4HDgceAj4DDABnAyc0a63Z6zkDmDPcV4HbALO6Vi/o/16EngvsBB4DfAG4De6jvvzwKuBZe2u5T8BV7U/813g7I59Pwh8EXgpsBj46x4/TwyZBE11e1o15wP3AQ9X/NwWoLNrdJakp4BHaXXHfsH2012f2U0rNE4FZPte24/0OMaNtjfYnmgf6wLgt20/Z/sx4CPAqva+d/CjYHkt8KGO9XPa27F9t+27bE/Y/h7wyY799viQ7Sds/y/wJmCj7dts7wb+qv0zdv5MxwPHtlt8X+vx88SQSdBUdzPwVuBSenSbprEIeKJj/S7bR9heaPss2//R/QHbXwI+BlwH/EDS9ZLm9zjG5o7XxwMHAY+0uz1P0QqJl7e33wG8VtLRwCjwWeDs9qD2AuDbAJJOkfQFSY9Kegb4U1otlb0d99jOdbfu1u3c/gFaXalvStog6Vd7/DwxZBI0Fdl+iNag8JuAz1X5jKTDgPOAr+7D8T5q+0zgNFpdqPf32r3j9WZgJ7CwHWhH2J5v+7R23TFaYznvAb7S7t49Sqvr9bX22BHAJ2i13Jbang/8Ph1jLtMc9xFgyZ4VSepct/2o7XfZPhb4NeDjkk6u8m8RzZegmZnLgJ+x/VyvnSQd3B5w/TzwJPC3MzmIpFdJerWkg4DngB20xkz6anexvgh8WNJ8SSOSXiGps9tzB3AFPxqPub1rHVpdt2eAZyWdCvx6n0P/C3CapF9sjz+9Bzi642e6WNLi9uqTtEKq0s8UzZegmQHb37W9vscuH5C0nVZX6SbgbuCn+wXTNOYDn6L1B/kQrYHgv5zB598OzAU2tmvcBhzTsf0OWkHylb2sA/wura7i9va59LxKZHsrcDHwZ+3zXQrc2bHLq4BvSHoWWA38lu0HZ/AzRYMpD76KiNLSoomI4hI0EfE8STe0J4p+Zy/bJemjksbaE0l/qkrdBE1EdLoRWNlj+wW0xt+W0rpS+YkqRRM0EfE821/hhfO+ul0E3OSWu4AjJB3TY38Aet4MN/egQz1v3hEzO9MqGjT+rJ27ChUulPEj3VNdatKg3xkTE0XK+iUHF6m7/dktW20fta+f/9nXH+ptT1SbKXD3PTs30Jouscf1tq+fweEW8cKJmOPt93rNXO8dNPPmHcGKn+y+vWX/aXeh6RMF/shGx6reaTAzmjevSF0fUuaPQbvL/PGWMLXtySJ1J37iFUXqfumrVz20P5/f9sQk31x7XKV9R495YIft5ftxuOn+yPr+byi390c0nIEppvruV5NxOmZ807pBdku/D2WMJqLhjNntyUpLDVYDb29ffToLeLrPDb9AWjQRQ6GuFo2kf6D1GJGFksaBP6R1ky62/wZYQ+t+vz33zL3oWUvTSdBENJwxkzXN8Ld9SZ/tBt4907oJmoghMDXglwUTNBEN17oNPkETEYWlRRMRRRnYPeBPYUjQRDSccbpOEVGYYXKwcyZBE9F0rZnBgy1BE9F4YnLaW5AGR4ImouFag8EJmogoqDWPJkETEYVNpUUTESWlRRMRxRkxOeBPfEnQRAyBdJ0ioigjdnn0QJ9GTwmaiIZrTdhrcNdJu6eY8/j22g86sfCw2msCjOyuf36k5pTJ4qmt24rUnTzz1CJ1D3rkqSJ1JxccWntNPba19poAo+vuLVK3DhkMjoiibDHpBrdoIqIZptKiiYiSWoPBg/2nPNhnFxF9NX4wOCKaYTLzaCKipMwMjohZMZWrThFRUuumygRNRBRkxO7cghARJdlkwl5ElKZM2IuIskxaNBExCzIYHBFFGQ38g68GOwYjoq/W163MqbRUIWmlpPsljUm6cprtx0n6sqRvSbpH0pv61UzQRDRe6wvkqix9K0mjwHXABcAy4BJJy7p2uwq41fYZwCrg4/3qpusU0XCm1pnBK4Ax25sAJN0CXARs7Drk/PbrBcCWfkUTNBFDYAZP2FsoaX3H+vW2r+9YXwRs7lgfB17dVeOPgC9K+k3gUOC8fgdN0EQ0nK2ZtGi22l7eY/t0ieWu9UuAG21/WNJrgJslnW57r8/STdBENFxrMLi2WxDGgSUd64t5cdfoMmAlgO2vS5oHLAQe21vRDAZHNF7rmcFVlgrWAUslnShpLq3B3tVd+3wfeAOApFcC84DHexXt2aLxzp1M/s93q5zcjIwcflrtNQE02d3Cq6NoofkJp55UpOyc/95UpO7kzp1F6o4cckL9RQt9c4WXlfmdsW7/Pt4aDK7nv1PbE5KuANYCo8ANtjdIugZYb3s18DvApyS9t334S233/ONL1yliCNQ5M9j2GmBN13tXd7zeCJw9k5oJmoiGa8LM4ARNxBDIw8kjoigbdk8laCKioFbXKUETEYXlu7cjoqg6L2+XkqCJaLx0nSJiFuSZwRFRVOuqU75uJSIKyoS9iJgV6TpFRFG56hQRsyJXnSKiKFtMJGgiorR0nSKiqIzRRMSsSNBERFGZRxMRs6LR82h0yDxGTu3+Nsz9p+07aq8J4O8/XH/NBfP777QPtGm8SF0WH12krO+r/yH1AHrmudprTiw7ofaaAGs/d1ORuqPH7N/nbZjIg68iorR0nSKiqIzRRMSscIImIkpr9GBwRAw+O2M0EVGcmMxVp4goLWM0EVFU7nWKiPLcGqcZZAmaiCEw6FedBnsEKSL6cnswuMpShaSVku6XNCbpyr3s80uSNkraIOnv+9VMiyZiCNTVdZI0ClwHnA+MA+skrba9sWOfpcDvAWfbflLSy/vVTYsmYgjYqrRUsAIYs73J9i7gFuCirn3eBVxn+8nWsf1Yv6IJmoiGs2cUNAslre9YLu8qtwjY3LE+3n6v0ynAKZLulHSXpJX9zjFdp4ghMIPL21ttL++xfbpC3R2zOcBS4FxgMfBVSafbfmpvRdOiiRgCdrWlgnFgScf6YmDLNPv8s+3dth8E7qcVPHuVoIloOCOmpkYqLRWsA5ZKOlHSXGAVsLprn88DrweQtJBWV2pTr6IJmogh4IpL3zr2BHAFsBa4F7jV9gZJ10i6sL3bWmCbpI3Al4H3297Wq27GaCKazvXe62R7DbCm672rO14beF97qSRBEzEMcgtCRJTW6Lu3vWMn3jBW+0EnV9T/zQoAc+YcV3/RH2ytvybA6GiZupNTRcqOnN7zosK+e3yvV0T3me6+r/aaAG+8+NIideGq/fq0gampBgdNRDSAgSa3aCKiGfKYiIgoL0ETEWVVvmHygEnQRAyDtGgioiiDc9UpIspL0EREaek6RURxCZqIKCoT9iJiNmTCXkSUl6tOEVGa0qKJiKKqPj7vAErQRDSeMhgcEbMgLZqIKK7M885qk6CJaLrMo4mI2ZCrThFR3oAHTb5ALiKK69mi0egoIy87ovaD6tmdtdcEmJw/r/aac7a/pPaaALsXH1mk7kE/eLpIXe0o8zvzoYfUXnPk0EW11wQYffiJInXrkK5TRJRlcgtCRMyCtGgiorR0nSKivARNRBSXoImIkuTB7zplHk3EMJhStaUCSSsl3S9pTNKVPfZ7syRLWt6vZoImYgjsadX0W/rWkUaB64ALgGXAJZKWTbPf4cB7gG9UOb8ETcQwcMWlvxXAmO1NtncBtwAXTbPfB4FrgR1ViiZoIpquYmum3aJZKGl9x3J5V7VFwOaO9fH2e8+TdAawxPYXqp5iBoMjhkH1weCttnuNqUw3kPN8dUkjwEeASysfkQRNxFBQfQ++GgeWdKwvBrZ0rB8OnA7cLgngaGC1pAttr99b0XSdIqLTOmCppBMlzQVWAav3bLT9tO2Ftk+wfQJwF9AzZCBBEzEcahoMtj0BXAGsBe4FbrW9QdI1ki7c19NL1ymi6WqesGd7DbCm672r97LvuVVqJmgihsGAzwxO0EQMgwRNRJQkar3qVESCJqLpGnBTZYImYhgkaCKiuEYHzegIzD+s9oN6bpl882iBaUG7d9dfExiZGPBOdRc/98MyhY98ae0lp763uf9O++L0k8vUfXD/S6TrFBHlJWgioijnqlNEzIa0aCKitIzRRER5CZqIKKr6YzoPmARNRMOJdJ0iYhYkaCKivARNRBSXoImIonL3dkTMigRNRJSWWxAiorh0nSKirEzYi4hZkaCJiJIyMzgiZoWmBjtpEjQRTZcxmoiYDek6RUR5jQ6akRH8koNrP+joY0/VXhNg5Ic7aq/po+p/Sj/AyNh4kbp++ZFF6pb4tgIA7ar/WyZGTjqu9poAU/91f5G6dUiLJiLKG/CgKfBFSBExq9rfglBlqULSSkn3SxqTdOU0298naaOkeyT9p6Tj+9VM0EQ03J55NFWWvrWkUeA64AJgGXCJpGVdu30LWG77x4HbgGv71U3QRAwDu9rS3wpgzPYm27uAW4CLXngof9n2nq8uvQtY3K9ogiZiCNTVogEWAZ3fKTzefm9vLgP+tV/RDAZHNN3MJuwtlLS+Y/1629d3rGsvR3gRSW8DlgPn9DtogiZiCMzgeTRbbS/vsX0cWNKxvhjY8qLjSecBfwCcY3tnv4MmaCKGQI0PvloHLJV0IvAwsAp46wuOJZ0BfBJYafuxKkUTNBFNZ6oO9PYvZU9IugJYC4wCN9jeIOkaYL3t1cBfAIcB/ygJ4Pu2L+xVN0ETMQTqnBlsew2wpuu9qztenzfTmgmaiGEw4DODEzQRDZcHX0VEeXYefBURs2CwcyZBEzEM0nWKiLIMpOsUEcUNds4kaCKGQbpOEVFcrjpFRFmN/7qVqSn0XP0P/J58tNJ9WDPmnX1vIp25rVvrrwnozNPK1P3OWJG6nHpSkbLaUf/vTE9tr70mgAe01dCasDeY57ZHWjQRw6C+u7eLSNBEDIG0aCKirMaP0UREA+Rep4iYDek6RURRrvVRnkUkaCKGQVo0EVHcYOdMgiZiGGhqsPtOCZqIpjOZsBcRZQlnwl5EzIIETUQUl6CJiKIyRhMRsyFXnSKiMKfrFBGFmQRNRMyCwe45JWgihkHm0UREeQMeNCMH+gQiYj/ZMDlVbalA0kpJ90sak3TlNNsPlvTZ9vZvSDqhX80+34JgtGNXpZObiZGDD669JgBLjq295NTmLbXXBBjd+kyRul4wv0hdbStzvrsXH1l7zYPGt9VeE2DOcYuK1OXBGmrU1KKRNApcB5wPjAPrJK22vbFjt8uAJ22fLGkV8OfAW3rVTYsmYhjY1Zb+VgBjtjfZ3gXcAlzUtc9FwGfar28D3iBJvYomaCKazsCUqy2wUNL6juXyrmqLgM0d6+Pt96bdx/YE8DTQs2maweCIxjO48vXtrbaX99g+XcukuylUZZ8XSNBENJ2pPNBbwTiwpGN9MdA9ULlnn3FJc4AFwBO9iqbrFDEM6hujWQcslXSipLnAKmB11z6rgXe0X78Z+JLdu3haNBHDoKarTrYnJF0BrAVGgRtsb5B0DbDe9mrg08DNksZotWRW9auboIlovHpvqrS9BljT9d7VHa93ABfPpGaCJqLpDOQxERFR3IDfgpCgiWg813nVqYgETUTTGVx9Hs0BkaCJGAZT6TpFRGkZo4mIouxcdYqIWZAWTUSUZTw5eaBPoqcETUTT7XlMxABL0EQMg1zejoiSDDgtmogoyjN68NUBkaCJGAKDPhisXs+rkfQ48NDsnU7E/0vH2z5qXz8s6d+AhRV332p75b4ea1/1DJqIiDrkUZ4RUVyCJiKKS9BERHEJmogoLkETEcX9H7KpPbGYXqS/AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# h_PI example\n",
    "mdp = MDP(N = 10)\n",
    "policy, Vs = h_PI(mdp, h = 3)\n",
    "V = Vs[-1]\n",
    "mdp.displayer.display_rewards(title = \"MDP's rewards\")\n",
    "mdp.displayer.display_actions(mdp.utils.greedy_policy_to_actions(policy), title = \"h-greedy's calculated best actions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
