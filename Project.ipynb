{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RL Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-07T19:19:54.495811Z",
     "start_time": "2021-12-07T19:19:54.127857Z"
    }
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "import matplotlib.patches as mpatches\n",
    "from enum import IntEnum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment & MDP methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-12-08T00:00:55.615Z"
    }
   },
   "outputs": [],
   "source": [
    "class Action(IntEnum):\n",
    "    STAY = 0\n",
    "    UP = 1\n",
    "    DOWN = 2\n",
    "    LEFT = 3\n",
    "    RIGHT = 4\n",
    "\n",
    "class MDP:\n",
    "    \n",
    "    # How to use :\n",
    "    #\n",
    "    # Use mdp.call(state, action) to get info on the environment.\n",
    "    # --> Do NOT use T & R matrices to get info on the environnement.\n",
    "    # --> You CAN use mdp.utils methods, as long as it has a update_n_calls parameter.\n",
    "    # Use mdp.step(action) to move the agent .\n",
    "    # Use mdp.V to store & update state values.\n",
    "    # Use mdp.n_calls to get the number of calls made to the environnement.\n",
    "    # Use mdp.reset() when the episode has ended to start over.\n",
    "\n",
    "    def __init__(self, N = 5, discount_factor = 0.97, reward_range = 0.1):\n",
    "        self.N = N\n",
    "        self.n_states = N * N\n",
    "        self.discount_factor = discount_factor\n",
    "        self.reward_range = reward_range\n",
    "        self.reset()\n",
    "        self.T = self.generate_transition_matrix(N)\n",
    "        self.displayer = MDP_Displayer(self)\n",
    "        self.utils = MDP_Utils(self)\n",
    "    \n",
    "    def generate_transition_matrix(self, N):\n",
    "        transition_matrix = np.zeros((N * N, len(Action)), dtype=int)\n",
    "        for s, state in enumerate(transition_matrix):\n",
    "            state[Action.UP] = s - N if s >= N else s\n",
    "            state[Action.DOWN] = s + N if s < N * N - N else s\n",
    "            state[Action.LEFT] = s - 1 if s % N else s\n",
    "            state[Action.RIGHT] = s + 1 if (s + 1) % N else s\n",
    "            state[Action.STAY] = s\n",
    "        return transition_matrix\n",
    "    \n",
    "    def generate_rewards(self, N, reward_range):\n",
    "        rewards = np.random.uniform(-reward_range, reward_range, N * N)\n",
    "        rewards[np.random.randint(0, N * N)] = 1\n",
    "        return rewards\n",
    "    \n",
    "    def generate_state_values(self, N):\n",
    "        return np.random.normal(size = N * N)\n",
    "    \n",
    "    def reset(self):\n",
    "        self.n_calls = 0\n",
    "        self.state = 0 # Paper does no mention what state to start on: 0 or random ?\n",
    "        self.R = self.generate_rewards(self.N, self.reward_range)\n",
    "        self.V = self.generate_state_values(self.N)\n",
    "    \n",
    "    def step(self, action):\n",
    "        self.state = self.T[self.state, action]\n",
    "        return self.state, self.R[self.state]\n",
    "    \n",
    "    def call(self, state, action):\n",
    "        n_calls += 1\n",
    "        new_state = self.T[state, action]\n",
    "        return new_state, self.R[new_state]\n",
    "        \n",
    "class MDP_Displayer:\n",
    "    \n",
    "    def __init__(self, mdp):\n",
    "        self.mdp = mdp\n",
    "        self.set_action_colors()\n",
    "    \n",
    "    def display_rewards(self, numerical = False, colorbar = True, ticks = False, title = \"Rewards\"):\n",
    "        self.display_heatmap(self.mdp.R, numerical, colorbar, ticks, title)\n",
    "    \n",
    "    def display_state_values(self, numerical = False, colorbar = True, ticks = False, title = \"State values\"):\n",
    "        self.display_heatmap(self.mdp.V, numerical, colorbar, ticks, title)\n",
    "        \n",
    "    def display_actions_based_on_rewards(self, ticks = False, title = \"Best actions based on rewards\"):\n",
    "        self.display_actions(self.mdp.utils.get_best_actions_based_on_rewards(False), ticks, title)\n",
    "    \n",
    "    def display_actions_based_on_state_values(self, ticks = False, title = \"Best actions based on state values\"):\n",
    "        self.display_actions(self.mdp.utils.get_best_actions_based_on_state_values(), ticks, title)\n",
    "    \n",
    "    def display_heatmap(self, array, numerical = False, colorbar = True, ticks = False, title = \"Heatmap\"):\n",
    "        data = np.reshape(array, (self.mdp.N, self.mdp.N))\n",
    "        if not ticks: self.remove_ticks()\n",
    "        if numerical: self.add_numerical(data)\n",
    "        plt.imshow(data)\n",
    "        plt.title(title)\n",
    "        if colorbar: plt.colorbar()\n",
    "        plt.show()\n",
    "        \n",
    "    def display_actions(self, actions, ticks = False, title = \"Actions\"):  \n",
    "        self.add_action_legend()\n",
    "        data = np.reshape(actions, (self.mdp.N, self.mdp.N))\n",
    "        if not ticks: self.remove_ticks()\n",
    "        plt.imshow(data, cmap = self.action_cmap, norm = self.action_norm)\n",
    "        plt.title(title)\n",
    "        plt.show()\n",
    "    \n",
    "    def remove_ticks(self):\n",
    "        plt.gca().set_xticklabels([])\n",
    "        plt.gca().set_yticklabels([])\n",
    "        for tick in plt.gca().xaxis.get_major_ticks():\n",
    "            tick.tick1line.set_visible(False)\n",
    "            tick.tick2line.set_visible(False)\n",
    "        for tick in plt.gca().yaxis.get_major_ticks():\n",
    "            tick.tick1line.set_visible(False)\n",
    "            tick.tick2line.set_visible(False)\n",
    "    \n",
    "    # Adds numerical data inside each cell of the plot\n",
    "    def add_numerical(self, data):\n",
    "        image = plt.imshow(data)\n",
    "        threshold = image.norm(data.max()) * 2/3\n",
    "        for i in range(len(data)):\n",
    "            for j in range(len(data[i])):\n",
    "                color = int(image.norm(data[i, j]) < threshold)\n",
    "                text = plt.gca().text(j, i, round(data[i, j], 2),\n",
    "                                      ha = \"center\", va = \"center\", color = str(color))\n",
    "    \n",
    "    def add_action_legend(self):\n",
    "        handles = []\n",
    "        for i, action in enumerate(Action):\n",
    "            handles.append(mpatches.Patch(color = self.action_colors[i],\n",
    "                                          label = str(action)[str(action).find(\".\") + 1:]))\n",
    "        plt.legend(title=\"Actions\", handles = handles, loc='upper left', bbox_to_anchor=(1, 1))\n",
    "        \n",
    "    def set_action_colors(self):\n",
    "        self.action_colors = [\"purple\", \"red\", \"blue\", \"green\", \"yellow\"]\n",
    "        self.action_cmap = mcolors.ListedColormap(self.action_colors)\n",
    "        self.action_norm = mcolors.BoundaryNorm(list(range(len(Action) + 1)), self.action_cmap.N)\n",
    "        \n",
    "class MDP_Utils:\n",
    "    \n",
    "    def __init__(self, mdp):\n",
    "        self.mdp = mdp\n",
    "        \n",
    "    # GENERATIVE METHODS ========================================================================\n",
    "        \n",
    "    # Warning: depth > 9 will most likely crash your kernel\n",
    "    def get_action_trajecories(self, depth):\n",
    "        return np.array(np.meshgrid(*[Action] * depth)).T.reshape(-1, depth)\n",
    "    \n",
    "    def get_uniform_policy(self):\n",
    "        return np.ones((self.mdp.n_states, len(Action)), dtype=int) / len(Action)\n",
    "    \n",
    "    def get_greedy_policy(self):\n",
    "        return self.actions_to_greedy_policy(self.get_best_actions_based_on_rewards())\n",
    "        \n",
    "    # INFORMATIVE METHODS =======================================================================\n",
    "    \n",
    "    def get_best_actions_based_on_rewards(self, update_n_calls = True):\n",
    "        return np.argmax(self.get_action_rewards(update_n_calls), 1)\n",
    "    \n",
    "    def get_best_actions_based_on_state_values(self):\n",
    "        return np.argmax(self.get_action_state_values(), 1)   \n",
    "    \n",
    "    # Return rewards array of size (n_state, actions)\n",
    "    def get_action_rewards(self, update_n_calls = True):\n",
    "        action_rewards = self.mdp.R[self.mdp.T]\n",
    "        if update_n_calls: self.mdp.n_calls += self.mdp.n_states * len(Action)\n",
    "        return action_rewards\n",
    "    \n",
    "    # Return state values array of size (n_state, actions), aka the value of the next state\n",
    "    def get_action_state_values(self):\n",
    "        action_state_values = self.mdp.V[self.mdp.T]\n",
    "        return action_state_values\n",
    "    \n",
    "    # CONVERSION METHODS ========================================================================\n",
    "        \n",
    "    def actions_to_greedy_policy(self, best_actions):\n",
    "        return np.eye(len(Action))[best_actions]\n",
    "    \n",
    "    def greedy_policy_to_actions(self, policy):\n",
    "        return np.argmax(policy, 1)\n",
    "    \n",
    "    # top_left=(0,0) any_coord=(row, column)\n",
    "    def state_to_coord(self, state):\n",
    "        return (int(np.floor(state / self.mdp.N)), state % self.mdp.N)\n",
    "    \n",
    "    # top_left=(0,0) any_coord=(row, column)\n",
    "    def coord_to_state(self, coord):\n",
    "        return coord[0] * self.mdp.N + coord[1]\n",
    "    \n",
    "    # BELLMAN METHODS =====================================================================\n",
    "    #\n",
    "    # check out this link for a better explanation of R, P, V & T than the paper's\n",
    "    # https://ai.stackexchange.com/questions/11057/what-is-the-bellman-operator-in-reinforcement-learning\n",
    "    #\n",
    "    # All the R, P = None is to make sure all functions can be called in a loop without recomputing R, P\n",
    "    # Same thing for returning R, P\n",
    "    \n",
    "    # Corresponds to bellman's P^π in the paper\n",
    "    def get_bellman_transition_kernel(self, policy):\n",
    "        P = np.zeros((self.mdp.n_states, self.mdp.n_states))\n",
    "        for s, state_transition in enumerate(self.mdp.T):\n",
    "            for action in Action:\n",
    "                P[s, state_transition[action]] += policy[s, action] # Should we update n_calls?\n",
    "        return P\n",
    "    \n",
    "    # Corresponds to bellman's r^π in the paper\n",
    "    def get_bellman_rewards(self, policy, update_n_calls = True):\n",
    "        if update_n_calls: self.mdp.n_calls += self.mdp.n_states * len(Action)\n",
    "        return np.sum(self.mdp.R[self.mdp.T] * policy, 1)\n",
    "    \n",
    "    # Corresponds to T^π in the paper\n",
    "    def bellman_operator(self, R = None, P = None, V = None, policy = None, update_n_calls = True):\n",
    "        if policy is None: policy = self.get_uniform_policy()\n",
    "        if R is None: R = self.get_bellman_rewards(policy, update_n_calls)\n",
    "        if P is None: P = self.get_bellman_transition_kernel(policy)\n",
    "        if V is None: V = self.mdp.V\n",
    "        V = R + self.mdp.discount_factor * P.dot(V)\n",
    "        return R, P, V\n",
    "    \n",
    "    # Corresponds to T & T_x in the paper\n",
    "    # OLD VERSION, KEEP IN CASE I FUCKED UP, BUT IM PRETTY SURE THIS WAS THE PROBLEM,\n",
    "    # I tried to fit the paper's definition but it didn't work apparently\n",
    "    #def bellman_optimatlity_operator(self, R = None, P = None, V = None, policy = None, update_n_calls = True):\n",
    "    #    R, P, V = self.bellman_operator(R, P, V, policy, update_n_calls)\n",
    "    #    V = np.max(V[self.mdp.T], 1)\n",
    "    #    return R, P, V\n",
    "    \n",
    "    # Corresponds to T & T_x in the paper\n",
    "    # equivalent of T* in https://ai.stackexchange.com/questions/11057/what-is-the-bellman-operator-in-reinforcement-learning\n",
    "    def bellman_optimatlity_operator(self, R = None, V = None, update_n_calls = True):\n",
    "        if V is None: V = self.mdp.V\n",
    "        if R is None: R = self.mdp.R[self.mdp.T] # Warning: This is ot the same R as bellman_operator ! Don't mix them up\n",
    "        if update_n_calls: self.mdp.n_calls += self.mdp.n_states * len(Action)\n",
    "        Pv = V[self.mdp.T]\n",
    "        V = np.max(R + self.mdp.discount_factor * Pv, 1) # V[self.mdp.T] is the same as P^a*v\n",
    "        return R, V\n",
    "    \n",
    "    # Corresponds to G & G_h in the paper\n",
    "    def get_h_greedy_policy(self, h = 1, R = None, P = None, V = None, policy = None, update_n_calls = True):\n",
    "        for i in range(h - 1):\n",
    "            R, V = self.bellman_optimatlity_operator(R, V, update_n_calls)\n",
    "        R, P, V = self.bellman_operator(V = V, update_n_calls = update_n_calls)\n",
    "        best_actions = np.argmax(V[mdp.T], 1)\n",
    "        policy = self.actions_to_greedy_policy(best_actions)\n",
    "        return policy, V\n",
    "    \n",
    "    def bellman_optimatlity_operator2(self, R = None, P = None, V = None, policy = None, update_n_calls = True):\n",
    "        R, P, V = self.bellman_operator(R, P, V, policy, update_n_calls)\n",
    "        V = np.max(V[self.mdp.T], 1)\n",
    "        return R, P, V\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-12-08T00:00:56.366Z"
    }
   },
   "outputs": [],
   "source": [
    "TOLERANCE = 1e-7\n",
    "\n",
    "def value_or_policy_iteration(mdp = MDP(), tolerance = TOLERANCE):\n",
    "    pass\n",
    "\n",
    "def h_PI(mdp, h = 1, tolerance = TOLERANCE):\n",
    "    if mdp.discount_factor < 0 or mdp.discount_factor >= 1:\n",
    "        raise ValueError(\"h_PI() will not converge unless discount_factor ∈ [0, 1[\")\n",
    "    V = mdp.V\n",
    "    while True:\n",
    "        policy, V = mdp.utils.get_h_greedy_policy(h = h, V = V)\n",
    "        if np.allclose(Vs[-1], Vs[-2], tolerance): break\n",
    "    return policy, V\n",
    "\n",
    "def NC_hm_PI(mdp, h = 1, m = 1, tolerance = TOLERANCE):\n",
    "    pass\n",
    "\n",
    "def hm_PI(mdp, h = 1, m = 1, tolerance = TOLERANCE):\n",
    "    pass\n",
    "\n",
    "def NC_hλ_PI(mdp, h = 1, λ = 1, tolerance = TOLERANCE):\n",
    "    pass\n",
    "\n",
    "def hλ_PI(mdp, h = 1, λ = 1, tolerance = TOLERANCE):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-12-08T00:00:57.036Z"
    }
   },
   "outputs": [],
   "source": [
    "# h_PI example\n",
    "mdp = MDP(N = 10)\n",
    "policy, V = h_PI(mdp, h = 30)\n",
    "mdp.displayer.display_rewards(title = \"MDP's rewards\")\n",
    "mdp.displayer.display_heatmap(V)\n",
    "mdp.displayer.display_actions(mdp.utils.greedy_policy_to_actions(policy), title = \"h-greedy's calculated best actions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
